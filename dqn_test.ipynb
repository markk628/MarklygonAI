{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "633aba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot  as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from src.config.config import DATA_DIR\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7649ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTradingEnv:\n",
    "    def __init__(self, data, initial_balance=10000, transaction_fee=0.0015, window_size=20):\n",
    "        self.data = data\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_fee = transaction_fee # 거래 수수료 (0.15%)\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # 상태 정규화를 위한 스케일러\n",
    "        self.scaler = StandardScaler()\n",
    "        # 처음 100개 데이터로 스케일러 학습\n",
    "        features = self._get_features(self.window_size)\n",
    "        self.scaler.fit(features[:100])\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = self.window_size\n",
    "        self.balance = self.initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.total_shares_bought = 0\n",
    "        self.total_shares_sold = 0\n",
    "        self.total_cost = 0\n",
    "        self.total_sales = 0\n",
    "\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_features(self, start_idx) -> np.array:\n",
    "        features = []\n",
    "        for i in range(start_idx - self.window_size, start_idx):\n",
    "            row = [self.data.iloc[i][col] for col in self.data.columns]\n",
    "            price_change = (self.data.iloc[i]['close'] - self.data.iloc[i-1]['close']) / self.data.iloc[i-1]['close'] if i > 0 else 0\n",
    "            row.append(price_change)\n",
    "            features.append(row)\n",
    "        return np.array(features)\n",
    "\n",
    "    def _get_state(self):\n",
    "        # 현재 상태 정보 구성\n",
    "        features = self._get_features(self.current_step)\n",
    "\n",
    "        # 정규화\n",
    "        normalized_features = self.scaler.transform(features)\n",
    "\n",
    "        # 포트폴리오 정보 추가\n",
    "        current_price = self.data.iloc[self.current_step]['close']\n",
    "        portfolio_value = self.balance + self.shares_held * current_price\n",
    "        portfolio_info = np.array([\n",
    "            portfolio_value,\n",
    "            self.balance / self.initial_balance, # 현재 잔고 비율\n",
    "            self.shares_held * current_price / self.initial_balance, # 보유 주식 가치 비율\n",
    "            self.shares_held > 0, # 주식 보유 여부 (불리언)\n",
    "        ])\n",
    "\n",
    "        # 상태를 1D 배열로 변환\n",
    "        state = np.concatenate((normalized_features.flatten(), portfolio_info))\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        current_price = self.data.iloc[self.current_step]['close']\n",
    "        reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # 매도 (전량)\n",
    "        if action == 0 and self.shares_held > 0:\n",
    "            # 매도 금액 계산\n",
    "            sell_amount = self.shares_held * current_price\n",
    "            # 거래 수수료 계산\n",
    "            fee = sell_amount * self.transaction_fee\n",
    "            # 실제 매도 금액\n",
    "            self.balance += (sell_amount - fee)\n",
    "\n",
    "            # 매도 기록 업데이트\n",
    "            self.total_shares_sold += self.shares_held\n",
    "            self.total_sales += sell_amount\n",
    "\n",
    "            # 손익 계산 (매도 금액 - 매수 비용 - 수수료)\n",
    "            profit = sell_amount - (self.shares_held * (self.total_cost / self.total_shares_bought if\n",
    "            self.total_shares_bought > 0 else 0)) - fee\n",
    "\n",
    "            # 보상 설정 (수익률)\n",
    "            reward = profit / self.initial_balance\n",
    "\n",
    "            # 주식 보유량 초기화\n",
    "            self.shares_held = 0\n",
    "\n",
    "        # 보유 (아무 행동 안함)\n",
    "        elif action == 1:\n",
    "            # 작은 음의 보상 (시간 비용)\n",
    "            reward = -0.0001\n",
    "\n",
    "        # 매수 (가능한 최대 수량)\n",
    "        elif action == 2 and self.balance > 0:\n",
    "            # 최대 구매 가능 주식 수 계산 (수수료 포함)\n",
    "            max_shares = self.balance / (current_price * (1 + self.transaction_fee))\n",
    "            max_shares = int(max_shares) # 소수점 이하 버림\n",
    "\n",
    "            if max_shares > 0:\n",
    "                # 매수 금액 계산\n",
    "                buy_amount = max_shares * current_price\n",
    "                # 거래 수수료 계산\n",
    "                fee = buy_amount * self.transaction_fee\n",
    "                # 실제 매수 비용\n",
    "                cost = buy_amount + fee\n",
    "\n",
    "                # 잔고 업데이트\n",
    "                self.balance -= cost\n",
    "                # 보유 주식 업데이트\n",
    "                self.shares_held += max_shares\n",
    "\n",
    "                # 매수 기록 업데이트\n",
    "                self.total_shares_bought += max_shares\n",
    "                self.total_cost += cost\n",
    "\n",
    "                # 작은 음의 보상 (매수 직후에는 수수료만큼 손해)\n",
    "                reward = -fee / self.initial_balance\n",
    "            else:\n",
    "            # 잔고 부족으로 매수 불가\n",
    "                reward = -0.001\n",
    "\n",
    "       \n",
    "        # 다음 스텝으로 이동\n",
    "        self.current_step += 1\n",
    "\n",
    "        # 데이터 종료 여부 확인\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            done = True\n",
    "            # 마지막 스텝에서 모든 주식 매도하여 최종 성과 계산\n",
    "            if self.shares_held > 0:\n",
    "                sell_amount = self.shares_held * current_price\n",
    "                fee = sell_amount * self.transaction_fee\n",
    "                self.balance += (sell_amount - fee)\n",
    "                self.shares_held = 0\n",
    "\n",
    "            # 최종 수익률 계산\n",
    "            final_portfolio_value = self.balance\n",
    "            return_rate = (final_portfolio_value - self.initial_balance) / self.initial_balance\n",
    "\n",
    "            # 최종 보상에 전체 수익률 반영\n",
    "            reward += return_rate\n",
    "\n",
    "        # 다음 상태, 보상, 종료 여부 반환\n",
    "        next_state = self._get_state()\n",
    "\n",
    "        return next_state, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abadb8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1: nn.Linear = nn.Linear(state_size, 128)\n",
    "        self.fc2: nn.Linear = nn.Linear(128, 64)\n",
    "        self.fc3: nn.Linear = nn.Linear(64, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd5cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, \n",
    "                 state_size: int,\n",
    "                 action_size: int,\n",
    "                 learning_rate: float=0.001,\n",
    "                 discount_factor: float=0.95,\n",
    "                 epsilon: float=1.0,\n",
    "                 epsilon_decay: float=0.995,\n",
    "                 epsilon_min: float=0.01,\n",
    "                 batch_size: int=64,\n",
    "                 memory_size: int=2000):\n",
    "        self.state_size: int = state_size\n",
    "        self.action_size: int = action_size\n",
    "        self.memory: deque = deque(maxlen=memory_size)\n",
    "        self.batch_size: int = batch_size\n",
    "        self.discount_factor: float = discount_factor # gamma (γ)\n",
    "        self.epsilon: float = epsilon # epsilon (ε)\n",
    "        self.epsilon_decay: float = epsilon_decay\n",
    "        self.epsilon_min: float = epsilon_min\n",
    "        self.learning_rate: float = learning_rate\n",
    "\n",
    "        # main and target network\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.main_network: DQNNetwork = DQNNetwork(state_size, action_size).to(self.device)\n",
    "        self.target_network: DQNNetwork = DQNNetwork(state_size, action_size).to(self.device)\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "        self.target_network.eval() # target network doesn't get trained\n",
    "\n",
    "        self.optimizer: optim.Adam = optim.Adam(self.main_network.parameters(), lr=learning_rate)\n",
    "        self.loss_fn: nn.MSELoss = nn.MSELoss()\n",
    "\n",
    "        self.update_counter: int = 0\n",
    "        self.target_update_frequency: int = 100 # standard for how often target network gets updated\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # store experience in the experience replay memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state, training=True):\n",
    "        # select an action according to the ε-greedy policy\n",
    "        if training and np.random.rand() < self.epsilon:\n",
    "            # Exploration: Select a random action\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        # Exploitation: Select the optimal action according to the current policy\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        self.main_network.eval()\n",
    "        with torch.no_grad():\n",
    "            q_values = self.main_network(state)\n",
    "        self.main_network.train()\n",
    "        return torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "    def train(self):\n",
    "        # Skip training if there is not enough experience accumulated in memory\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # create sample batch\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        # ready batch data\n",
    "        states = torch.FloatTensor(np.array([experience[0] for experience in minibatch])).to(self.device)\n",
    "        actions = torch.LongTensor([[experience[1]] for experience in minibatch]).to(self.device)\n",
    "        rewards = torch.FloatTensor([[experience[2]] for experience in minibatch]).to(self.device)\n",
    "        next_states = torch.FloatTensor(np.array([experience[3] for experience in minibatch])).to(self.device)\n",
    "        dones = torch.FloatTensor([[experience[4]] for experience in minibatch]).to(self.device)\n",
    "\n",
    "        # calculate current q values\n",
    "        q_values = self.main_network(states).gather(1, actions)\n",
    "\n",
    "        # Calculate the maximum Q-value of the next state using the target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1, keepdim=True)[0]\n",
    "\n",
    "        # calculate target q values using Q-learning formula\n",
    "        target_q_values = rewards + (self.discount_factor * next_q_values * (1 - dones))\n",
    "\n",
    "        # calculate loss and execute backpropagation\n",
    "        loss = self.loss_fn(q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # decrease epsilon (decrease exploration probability)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # periodically update the target network\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update_frequency == 0:\n",
    "            self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "\n",
    "    def load(self, name):\n",
    "        self.main_network.load_state_dict(torch.load(name))\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.main_network.state_dict(), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d273cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env: StockTradingEnv, agent: DQNAgent, episodes: int=100):\n",
    "    scores = []\n",
    "    balances = []\n",
    "    \n",
    "    print('training...') \n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        done = False\n",
    "        count = 5759\n",
    "        \n",
    "        while not done:\n",
    "            step = 5760 - count\n",
    "            if step % 1000 == 0:\n",
    "                print(f'Step: {step}')\n",
    "            # choose action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # apply action to environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # save results\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            # train agent\n",
    "            agent.train()\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            count -= 1\n",
    "\n",
    "        scores.append(score)\n",
    "        balances.append(env.balance)\n",
    "        print(f\"Episode: {e+1}/{episodes}, Score: {score:.4f}, \"f\"Balance: {env.balance:.2f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "    return scores, balances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce8ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env: StockTradingEnv, agent: DQNAgent, episodes: int=10):\n",
    "    total_return = 0\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state, training=False) # evaluation mode\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "\n",
    "        # calculate total gain\n",
    "        return_rate = (env.balance - env.initial_balance) / env.initial_balance\n",
    "        total_return += return_rate\n",
    "\n",
    "        print(f\"Evaluation Episode {e+1}/{episodes}, Return: {return_rate:.4f}, \"f\"Final Balance: {env.balance:.2f}\")\n",
    "\n",
    "    avg_return = total_return / episodes\n",
    "    print(f\"Average Return: {avg_return:.4f}\")\n",
    "\n",
    "    return avg_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3105684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_stock_data(ticker: str) -> pd.DataFrame:\n",
    "#     drop_cols = ['timestamp', 'target']\n",
    "#     file_path = DATA_DIR / f'feature_engineered/{ticker.lower()}.csv'\n",
    "#     df = pd.read_csv(file_path)\n",
    "    \n",
    "#     if drop_cols:\n",
    "#         df.drop(drop_cols, axis=1, inplace=True)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "def load_stock_data(ticker: str) -> pd.DataFrame:\n",
    "    drop_cols = ['timestamp', 'target']\n",
    "    file_path = DATA_DIR / f'feature_engineered/{ticker.lower()}.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])  # ensure datetime format\n",
    "\n",
    "    # Define your cutoff datetime\n",
    "    cutoff = pd.Timestamp('2025-04-29 08:00:00', tz='UTC')\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    df = df[df['timestamp'] >= cutoff]\n",
    "    if drop_cols:\n",
    "        df.drop(drop_cols, axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64464411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "Step: 5000\n",
      "helloworld\n",
      "Episode: 1/100, Score: -6.0842, Balance: 1517.76, Epsilon: 0.0100\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "Step: 5000\n",
      "helloworld\n",
      "Episode: 2/100, Score: -5.2905, Balance: 1285.97, Epsilon: 0.0100\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n"
     ]
    }
   ],
   "source": [
    "data = load_stock_data('AAPL').loc[:,:'vwap']\n",
    "\n",
    "# create environment\n",
    "env = StockTradingEnv(data, initial_balance=10000, window_size=20)\n",
    "\n",
    "# define the size of the state and action space\n",
    "state = env.reset()\n",
    "state_size = len(state)\n",
    "action_size = 3 # sell(0), hold(1), buy(2)\n",
    "\n",
    "# initialize agent\n",
    "agent = DQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    learning_rate=0.001,\n",
    "    discount_factor=0.95,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay=0.995,\n",
    "    epsilon_min=0.01,\n",
    "    batch_size=1024,\n",
    "    memory_size=10000\n",
    ")\n",
    "# train agent\n",
    "num_episodes = 100\n",
    "scores, balances = train_agent(env, agent, episodes=num_episodes)\n",
    "\n",
    "# visualize training result\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(scores)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.title('DQN Learning Curve')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(balances)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Final Balance')\n",
    "plt.title('Portfolio Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dqn_stock_trading_results.png')\n",
    "plt.show()\n",
    "\n",
    "# save model\n",
    "agent.save(\"dqn_stock_model.pth\")\n",
    "\n",
    "# evaluate trained agent\n",
    "avg_return = evaluate_agent(env, agent, episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
